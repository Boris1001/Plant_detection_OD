Распознавание растений с помощью нейронных сетей.
Хочу поделиться опытом создания модели на основе принципа Object Detection в Google.colab для практического обнаружения и распознавания объектов (в данном случае дико вредного сорняка на плантациях пшеницы) с помощью пакета YOLOv7 на PyTorch и разметки данных в Roboflow.
Задачу поставил такую: научить модель распознавать “дымянку лекарственную” в полях сельхозкультур. 
На момент создания моей модели (январь 2023 г.)  уже вышла  YOLOv8, которая работает через pip install, но как показала практика работы с версиями от YOLOv3 до YOLOv7 - нужно несколько месяцев с момента релиза на обкатку пользователями и совмещением со всеми фрейворками и библиотеками, а самое главное до насыщения интернета гневными туториалами описывающими возможности их совместной работы. Кроме того точность и скорость распознавания в 7-й и 8-й версиях не сильно отличаются и семёрка меня показалась более гибкой и удобной.
Этапы.
Итак, давайте по порядку с описанием этапов / инструментов:
Сбор базы для обучения  / камера на смартфоне и квадрокоптер подаренный сыну на день рождения. ))
Разметка данных / сервис Roboflow
Создание модели на python / блокнот google.colab
Подключение облака для работы с данными / google.drive
Инсталляция предобученной модели YOLOv7 в ваше облако / GitHub через git clone
Загрузка подготовленного (размеченного) датасета с платформы Roboflow в облако.
Дообучение модели на собственном датасете
Получение Тестовых результатов.
Обнаружение объектов на фото/видео.

Сбор базы
Сразу следует заметить что правильно собранная база - это 90% успеха (про метрику измерения процентов не спрашивайте - секрет автора!)
Попытки наковырять базу с просторов интернета в нужном количестве и качестве быстро подсказали единственное правильное решение - брать смартфон, бутерброды, термос с чаем и отправляться дышать свежим воздухом в поисках посевных. 
По факту за день удалось наснимать около 1300 фото. Не так страшен чёрт, как его малюют. После чистки и обработки получилось около 500 фотографий приемлемого качества:
 и несколько видео с высот от 1 до 3 метров снятых с простенького дрона для тестов. 
Для обучения модели на кадрах с видео для съёмки нужна игрушка помощнее. 
Разметка данных
Приятным сюрпризом оказалось открытие платформы Roboflow для предобработки данных для моделей Object Detection. Чтобы получить готовые датасеты нужно:
загрузить фото на платформу
разметить фото с помощью мышки. Либо выделяете требуемым объект просто прямоугольной рамкой, либо заморачиваетесь и обводите сложный контур.
выбираете способы аугментации (градации серого, поворот, масштабирование и т.д.) при необходимости
здесь же задаете процентное соотношение обучающей/тестовой/проверочной выборки
выбираете модель для которой нужен датасет:
получаете ссылку на готовый датасет (фото/лейбл в формате train/test/valid 


Лейбл - это файл txt, в котором записан класс изучаемого объекта и его координаты на фото. 
Roboflow позволяет выполнить разметку просто с помощью мышки и терпения без каких-либо навыков в графических редакторах. Задать границы объекта можно прямоугольной рамкой - это пять значений в лейбле: 
0 0.5140625 0.39609375 0.5515625 0.6421875
либо по контуру и тогда получите значения крайних точек:
0 0.5140625 0.39609375 0.5515625 0.6421875 0.446875 0.8375 0.384375 0.325 0.9109375 0.50234375 0.178125 0.4671875
Весь процесс (после регистраций и знакомления с инструментами) занимает несколько часов.
Разметка в Roboflow принесла несколько сюрпризов: 
Этапы аугментация связанные с пространственными изменениями такие как isolate object, static crop, rotation, flip нарушали конечную разметку. После обучения при получении очень низкой точности распознавания пришлось писать код для проверки лейблов. Roboflow в качестве датасета выдаёт каталог с фото и каталог с лейблами в относительных координатах, поэтому визуально проверить нереально. То есть в Roboflow всё супер - генерируешь датасет - а там в итоге всё совсем плохо. 
На разметку влияет пространственное расположение фото. У меня была часть вертикальных и часть горизонтальных. При генерации датасета часть данных выходила с отзеркаленными лейблами.
Достучаться до техподдержки с данными проблемами не получилось. В течении нескольких недель никто не ответил.
	Хорошо сработала предобработка в градации серого.
К плюсам можно отнести интуитивный интерфейс, лёгкую обработку изображений, автоматическое создание датасета и удобную загрузку. 
Модель
Я работал в тетрадке collab.research.google.com, который любезно бесплатно всё ещё предоставляет мощности графических процессоров для обучения нейронок и позволяет напрямую работать с ветками GitHub
Создаём новый блокнот collab.research.google.com, подключаем среду выполнения с графическим процессором (выбираем слева сверху вкладку “среда выполнения”, и в разделе аппаратный ускоритель выбираем GPU.

Подключаем Google disk для работы с данными и сохранения результатов
from google.colab import drive
drive.mount('/content/drive')
Устанавливаем предобученную YOLOv7 с GitHub, меняем прописываем путь к папке в которой собираемся работать и устананвливаем файл с зависимостями.
!git clone https://github.com/WongKinYiu/yolov7.git 
%cd yolov7
!pip install -r requirements.txt
Можем загрузить предобученные веса, либо начать с нуля.
!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt
Загружаем датасет с Roboflow в формате YOLOv7
pip install roboflow
from roboflow import Roboflow
rf = Roboflow(api_key="")         #Личный аpi_key удалён
project = rf.workspace("").project("medicinal-dymyanka")
dataset = project.version(24).download("yolov7")
 Данные для запуска просто копируем с платформы Roboflow
Сохраняем данные в переменную dataset
Запускаем обучение
!python train.py --epochs 300 --workers 4 --batch-size 16 \
--data {dataset.location}/data.yaml --img 640 640 --cfg cfg/training/yolov7.yaml \
--weights '' --name yolov7 --hyp data/hyp.scratch.p5.yaml
Указываем:
--epochs - количество проходов через нейронную сеть в прямом и обратном направлении
--workers  - количество движков, 
--batch-size – количество обучающих примеров за одну итерацию
--data  - путь к датасету (в представленном виде указывается путь к датасету загруженному в colab. В случае если сохранили датасет в облако указываем здесь путь к нему, но тогда нужно переписать data.yaml
--img - размер картинки при обработке
--cfg - путь к архитектуре модели
--weights - путь к весам к предобученной модели либо сохранённым весам с прошлого обучения 
--name  - имя каталога в каталоге runs в котором будут сохранять веса после обучения, результаты тестов и результаты распознавания.
--hyp  - гиперпараметры. Их можно переписать в установленной модели
Подробное описание и характеристики YOLOv7 лучше изучить в релизе её разработчиков. Уж очень красочно там всё разложено.
Запускаем модель на примере не участвующем в обучении
python detect.py --weights runs/train/yolov7_24d/weights/best.pt --conf 0.3 --source пример.JPG
--weights -  ставим путь к лучшим результатам обучения (они сохраняются автоматически).
--conf - задаём порог распознавания, подбирается опытным путём.
--source  - указываем путь к фото/видео, предварительно загруженному в рабочий каталог.
Смотрим как сработала модель:

 	При работе с видео модель не всегда определяла объекты другого этапа развития и объекты заслоненные другими растениями. На мой взгляд данные проблемы решаются увеличением базы для обучения с добавлением аугментации в виде фрагментации объектов

  

 4. Выводы.
При обработке данных собранных вручную выявилось много изображений ненадлежащего качества. В итоге в подготовку датасета пошло меньше половины. К фотосессии нужно подходить более тщательно и отдельное внимание уделить качеству оборудования. 
Roboflow - удобная интуитивная платформа для подготовки и загрузки датасета, но при использовании подобных инструментов нужно проверять данные непосредственно перед началом обучения модели.
YOLOv7 - полноценный готовый рабочий алгоритм для распознавания объектов. 
 	Копания в чертогах скриптов YOLO результатов не принесли. Несколько архитектур для разных задач представлены у них в релизе и более чем результативны.
Изменение гиперпараметров какого-либо существенного результата не принесли.
Обучение проводилось на размере изображений в 640х640. Увеличение до 1280х1280 в разы снижает скорость обучения, но может помочь в случае работы с мелкими объектами.

Итоговые параметры обучения которые принесли наилучший результат: 
обучение на 300 эпохах (где-то 20 эпох за один бесплатный сеанс colab) этапами с параметром –resume  который позволяет начинать с предыдущих весов. Веса каждой эпохи сохраняются и автоматически выбираются лучшие из них.
batch size 16. При увеличении более 16 CUDA выдавала ошибку связанную с лимитом мощностей.
шаг обучения по defaul в 0.001
Результаты при работе с фото - стабильно более 99% точности.





